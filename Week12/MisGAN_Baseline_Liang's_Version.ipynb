{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MisGAN_Baseline-Liang's Version",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CauA5zPe3e2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "544f512c-8953-4270-bb2f-e714ca24211e"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from matplotlib.patches import Rectangle\n",
        "import pylab as plt\n",
        "import os\n",
        "import urllib\n",
        "import pandas as pd\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "torch.set_printoptions(precision=2)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ka5AQpg7L_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9d897630-3a8d-42c8-b4c7-ab272dab65e6"
      },
      "source": [
        "# Load data\n",
        "DATA_PATH = \"https://raw.githubusercontent.com/ch00226855/ImputationGAN/master/Letter.csv\"\n",
        "DATA_FILE = \"Letter.csv\"\n",
        "\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    urllib.request.urlretrieve(DATA_PATH, DATA_FILE)\n",
        "\n",
        "raw_data = pd.read_csv(DATA_FILE, sep=\",\").to_numpy().astype(np.float32)\n",
        "print(\"Data shape:\", raw_data.shape)\n",
        "\n",
        "#divide the raw data into 4 subsets\n",
        "# Split Data into four pieces\n",
        "full_size = len(raw_data)\n",
        "n_subsets = 4\n",
        "sub_size = full_size // n_subsets\n",
        "\n",
        "subsets = []\n",
        "for k in range(4):\n",
        "    #subsets.append(raw_data[k*sub_size:(k+1)*sub_size, k:k+13])\n",
        "    subsets.append(raw_data[k*sub_size:(k+1)*sub_size, k*3:(k*3)+7])\n",
        "    \n",
        "print(\"Split data into\", n_subsets, \"subsets. Size of one subset:\", sub_size)\n",
        "subset0=subsets[0]\n",
        "subset1=subsets[1]\n",
        "subset2=subsets[2]\n",
        "subset3=subsets[3]\n",
        "# Normalization (0 to 1)\n",
        "Dim = subset0.shape[1]\n",
        "Min_Val = np.zeros(Dim)\n",
        "Max_Val = np.zeros(Dim)\n",
        "\n",
        "for i in range(Dim):\n",
        "    Min_Val[i] = np.min(subset3[:,i])\n",
        "    subset3[:,i] = subset3[:,i] - np.min(subset3[:,i])\n",
        "    Max_Val[i] = np.max(subset3[:,i])\n",
        "    subset3[:,i] = subset3[:,i] / (np.max(subset3[:,i]) + 1e-6)   \n",
        "\n",
        "subset3[:3]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape: (20000, 16)\n",
            "Split data into 4 subsets. Size of one subset: 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6 , 0.53, 0.62, 0.13, 0.69, 0.27, 0.6 ],\n",
              "       [0.27, 0.47, 0.31, 0.4 , 0.38, 0.13, 0.47],\n",
              "       [0.67, 0.6 , 0.69, 0.2 , 0.54, 0.2 , 0.4 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsyXCmtU8NFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedDataset(Dataset):\n",
        "    def __init__(self, subset, block_len, random_seed=0):\n",
        "        self.block_len = block_len\n",
        "        self.rnd = np.random.RandomState(random_seed)\n",
        "        self.data_size = len(subset)\n",
        "        self.num_features = len(subset[0])\n",
        "        self.generate_incomplete_data(subset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # return index so we can retrieve the mask location from self.mask_loc\n",
        "        return self.data[index], self.mask[index], index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "\n",
        "    def generate_incomplete_data(self, subset):\n",
        "        n_masks = self.data_size\n",
        "        self.data = [None] * n_masks\n",
        "        self.mask = [None] * n_masks\n",
        "        self.mask_loc = [None] * n_masks\n",
        "        for i in range(n_masks):\n",
        "            d0 = self.rnd.randint(0, self.num_features - self.block_len + 1)\n",
        "            mask = torch.zeros((self.num_features), dtype=torch.uint8)\n",
        "            mask[d0:(d0 + self.block_len)] = 1\n",
        "            self.mask[i] = mask.unsqueeze(0)   # add an axis for channel\n",
        "            self.mask_loc[i] = d0, self.block_len\n",
        "            # Mask out missing pixels by zero\n",
        "            self.data[i] = torch.from_numpy(subset[i]) * mask.float()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTri_8v38X2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create masked data\n",
        "masked_data = MaskedDataset(subset3, block_len=4)\n",
        "batch_size = 64\n",
        "data_loader = DataLoader(masked_data, batch_size=batch_size, shuffle=True,\n",
        "                         drop_last=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKPOhPl2B4ab",
        "colab_type": "text"
      },
      "source": [
        "Compare raw data and masked data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZB0kwBIAhgZ",
        "colab_type": "text"
      },
      "source": [
        "## MisGAN on Numerical Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93qW-XWpMOpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_data(data, mask, tau=0):\n",
        "    return mask * data + (1 - mask) * tau"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW9JbEKZCDZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size, DIM, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.DIM = DIM\n",
        "        self.latent_size = latent_size\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.preprocess=nn.Linear(latent_size, 4 * self.DIM)\n",
        "        self.block1=nn.Linear(4 * self.DIM, 2 * self.DIM)\n",
        "        self.block2=nn.Linear(2 * self.DIM, self.DIM+15)\n",
        "        self.final = nn.Linear(self.DIM+15, num_features)\n",
        "\n",
        "    def forward(self, input):\n",
        "        net = self.preprocess(input).clamp(min=0)\n",
        "        # net = net.view(-1, 4 * self.DIM, 4, 4)\n",
        "        net = self.block1(net).clamp(min=0)\n",
        "        # net = net[:, :, :7, :7]\n",
        "        net = self.block2(net).clamp(min=0)\n",
        "        # net = self.deconv_out(net)\n",
        "        net = self.final(net)\n",
        "        # return self.transform(net).view(-1, 1, 28, 28)\n",
        "        # return self.transform(net).view(-1, 1, self.num_features)\n",
        "        # return self.transform(net).view(-1, self.num_features)\n",
        "        return net\n",
        "\n",
        "\n",
        "class DataGenerator(Generator):\n",
        "    def __init__(self, latent_size, DIM, num_features):\n",
        "        super().__init__(latent_size, DIM, num_features)\n",
        "        self.transform = lambda x: torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class MaskGenerator(Generator):\n",
        "    def __init__(self, latent_size, DIM, num_features, temperature=.66):\n",
        "        super().__init__(latent_size, DIM, num_features)\n",
        "        self.transform = lambda x: torch.sigmoid(x / temperature)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efLxqwgFD_fT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, DIM, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.DIM = DIM\n",
        "        self.num_features = num_features\n",
        "        \n",
        "        self.preprocess=nn.Linear(num_features, self.DIM)\n",
        "        self.block1=nn.Linear(self.DIM, 2 * self.DIM)\n",
        "        self.block2=nn.Linear(2 * self.DIM, self.DIM+15)\n",
        "        self.final = nn.Linear(self.DIM+15, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        net = self.preprocess(input).clamp(min=0)\n",
        "        net = self.block1(net).clamp(min=0)\n",
        "        net = self.block2(net).clamp(min=0)\n",
        "        net = self.final(net)\n",
        "\n",
        "        return net.view(-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TilfFoiSFoN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CriticUpdater:\n",
        "    def __init__(self, critic, critic_optimizer, batch_size=64, gp_lambda=10):\n",
        "        self.critic = critic\n",
        "        self.critic_optimizer = critic_optimizer\n",
        "        self.gp_lambda = gp_lambda\n",
        "        # Interpolation coefficient\n",
        "        self.eps = torch.empty(batch_size, 1, device=device)\n",
        "        # For computing the gradient penalty\n",
        "        self.ones = torch.ones(batch_size).to(device)\n",
        "\n",
        "    def __call__(self, real, fake):\n",
        "        real = real.detach()\n",
        "        fake = fake.detach()\n",
        "        # print(\"real shape:\", real.shape)\n",
        "        # print(\"fake shape:\", fake.shape)\n",
        "        # print('eps shape:', self.eps.shape)\n",
        "        self.critic.zero_grad()\n",
        "        self.eps.uniform_(0, 1)\n",
        "        interp = (self.eps * real + (1 - self.eps) * fake).requires_grad_()\n",
        "        # print('interp shape:', interp.shape)\n",
        "        # print('critic(interp) shape:', self.critic(interp).shape)\n",
        "        # print('self.ones shape:', self.ones.shape)\n",
        "        grad_d = grad(self.critic(interp), interp, grad_outputs=self.ones,\n",
        "                      create_graph=True)[0]\n",
        "        grad_d = grad_d.view(real.shape[0], -1)\n",
        "        grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
        "        w_dist = self.critic(fake).mean() - self.critic(real).mean()\n",
        "        loss = w_dist + grad_penalty\n",
        "        loss.backward()\n",
        "        self.critic_optimizer.step()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJSaaEGbGBe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_critic = 5\n",
        "alpha = .2\n",
        "num_features = masked_data.num_features\n",
        "DIM = num_features # DIM determines layer sizes\n",
        "nz = num_features # dimensionality of the latent code\n",
        "latent_size = nz # number of random noise\n",
        "\n",
        "data_gen = DataGenerator(latent_size, DIM, num_features).to(device)\n",
        "mask_gen = MaskGenerator(latent_size, DIM, num_features).to(device)\n",
        "\n",
        "data_critic = Critic(DIM, num_features).to(device)\n",
        "mask_critic = Critic(DIM, num_features).to(device)\n",
        "\n",
        "data_noise = torch.empty(batch_size, nz, device=device)\n",
        "mask_noise = torch.empty(batch_size, nz, device=device)\n",
        "\n",
        "lrate = 1e-4\n",
        "data_gen_optimizer = optim.Adam(\n",
        "    data_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
        "mask_gen_optimizer = optim.Adam(\n",
        "    mask_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
        "\n",
        "data_critic_optimizer = optim.Adam(\n",
        "    data_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
        "mask_critic_optimizer = optim.Adam(\n",
        "    mask_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
        "\n",
        "update_data_critic = CriticUpdater(\n",
        "    data_critic, data_critic_optimizer, batch_size)\n",
        "update_mask_critic = CriticUpdater(\n",
        "    mask_critic, mask_critic_optimizer, batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZHm8kluGLT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "362ef4b7-27c5-4079-be15-5d2b68bbb092"
      },
      "source": [
        "plot_interval = 5\n",
        "critic_updates = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    for real_data, real_mask, _ in data_loader:\n",
        "\n",
        "        real_data = real_data.to(device).float()\n",
        "        real_mask = real_mask.view(real_data.shape).to(device).float()\n",
        "\n",
        "        # Update discriminators' parameters\n",
        "        data_noise.normal_()\n",
        "        mask_noise.normal_()\n",
        "\n",
        "        fake_data = data_gen(data_noise)\n",
        "        fake_mask = mask_gen(mask_noise)\n",
        "\n",
        "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
        "        masked_real_data = mask_data(real_data, real_mask)\n",
        "\n",
        "        # print(\"masked_real_data shape:\", masked_real_data.shape)\n",
        "        # print(\"masked_fake_data shape:\", masked_fake_data.shape)\n",
        "        # print(\"real_mask shape:\", real_mask.shape)\n",
        "        # print(\"fake_mask shape:\", fake_mask.shape)\n",
        "\n",
        "        update_data_critic(masked_real_data, masked_fake_data)\n",
        "        update_mask_critic(real_mask, fake_mask)\n",
        "\n",
        "        critic_updates += 1\n",
        "\n",
        "        if critic_updates == n_critic:\n",
        "            critic_updates = 0\n",
        "\n",
        "            # Update generators' parameters\n",
        "            for p in data_critic.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            for p in mask_critic.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "            data_gen.zero_grad()\n",
        "            mask_gen.zero_grad()\n",
        "\n",
        "            data_noise.normal_()\n",
        "            mask_noise.normal_()\n",
        "\n",
        "            fake_data = data_gen(data_noise)\n",
        "            fake_mask = mask_gen(mask_noise)\n",
        "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
        "\n",
        "            data_loss = -data_critic(masked_fake_data).mean()\n",
        "            mask_loss = -mask_critic(fake_mask).mean()\n",
        "            data_loss.backward(retain_graph=True)\n",
        "            (mask_loss + data_loss * alpha).backward()\n",
        "            data_gen_optimizer.step()\n",
        "            mask_gen_optimizer.step()\n",
        "\n",
        "            for p in data_critic.parameters():\n",
        "                p.requires_grad_(True)\n",
        "            for p in mask_critic.parameters():\n",
        "                p.requires_grad_(True)\n",
        "\n",
        "    if plot_interval > 0 and (epoch + 1) % plot_interval == 0:\n",
        "        print(\"Iteration:\", epoch + 1)\n",
        "        # Although it makes no difference setting eval() in this example, \n",
        "        # you will need those if you are going to use modules such as \n",
        "        # batch normalization or dropout in the generators.\n",
        "        data_gen.eval()\n",
        "        mask_gen.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # print('Epoch:', epoch)\n",
        "            # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 2.5))\n",
        "            \n",
        "            data_noise.normal_()\n",
        "            data_samples = data_gen(data_noise)\n",
        "            # plot_grid(ax1, data_samples, title='generated complete data')\n",
        "            #print(\"Generated data:\")\n",
        "            #print(data_samples)\n",
        "            \n",
        "            mask_noise.normal_()\n",
        "            mask_samples = mask_gen(mask_noise)\n",
        "            # plot_grid(ax2, mask_samples, title='generated masks')\n",
        "            #print(\"Generated mask:\")\n",
        "            #print(mask_samples)\n",
        "            \n",
        "            # plt.show()\n",
        "            # plt.close(fig)\n",
        "\n",
        "        data_gen.train()\n",
        "        mask_gen.train()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 5\n",
            "Iteration: 10\n",
            "Iteration: 15\n",
            "Iteration: 20\n",
            "Iteration: 25\n",
            "Iteration: 30\n",
            "Iteration: 35\n",
            "Iteration: 40\n",
            "Iteration: 45\n",
            "Iteration: 50\n",
            "Iteration: 55\n",
            "Iteration: 60\n",
            "Iteration: 65\n",
            "Iteration: 70\n",
            "Iteration: 75\n",
            "Iteration: 80\n",
            "Iteration: 85\n",
            "Iteration: 90\n",
            "Iteration: 95\n",
            "Iteration: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9eFitlVTdz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Imputer(nn.Module):\n",
        "    def __init__(self, num_features, arch):\n",
        "        super().__init__()\n",
        "        self.preprocess=nn.Linear(num_features, arch[0])\n",
        "        self.block1=nn.Linear(arch[0], arch[1])\n",
        "        self.block2=nn.Linear(arch[1], arch[0]+15)\n",
        "        self.final = nn.Linear(arch[0]+15, num_features)\n",
        "        \n",
        "    def forward(self, data, mask, noise):\n",
        "        net = data * mask + noise * (1 - mask)\n",
        "        net = net.view(data.shape[0], -1)\n",
        "        net = self.preprocess(net).clamp(min=0)\n",
        "        net = self.block1(net).clamp(min=0)\n",
        "        net = self.block2(net).clamp(min=0)\n",
        "        net = self.final(net)\n",
        "\n",
        "        net = torch.sigmoid(net).view(data.shape)\n",
        "        return data * mask + net * (1 - mask)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFn_6HsvTsNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imputer = Imputer(num_features, (num_features, num_features)).to(device)\n",
        "impu_critic = Critic(DIM, num_features).to(device)\n",
        "impu_noise = torch.empty(batch_size, num_features, device=device)\n",
        "\n",
        "imputer_lrate = 2e-4\n",
        "imputer_optimizer = optim.Adam(\n",
        "    imputer.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
        "impu_critic_optimizer = optim.Adam(\n",
        "    impu_critic.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
        "update_impu_critic = CriticUpdater(\n",
        "    impu_critic, impu_critic_optimizer, batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPPJVjn2XqAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "3b6601a2-2a82-44bf-d053-1b0b84d35b29"
      },
      "source": [
        "beta = .1\n",
        "plot_interval = 10\n",
        "critic_updates = 0\n",
        "import time\n",
        "t1=time.time()\n",
        "for epoch in range(150):\n",
        "    for real_data, real_mask, index in data_loader:\n",
        "\n",
        "        real_data = real_data.to(device).float()\n",
        "        real_mask = real_mask.view(real_data.shape).to(device).float()\n",
        "\n",
        "        masked_real_data = mask_data(real_data, real_mask)\n",
        "\n",
        "        # Update discriminators' parameters\n",
        "        data_noise.normal_()\n",
        "        fake_data = data_gen(data_noise)\n",
        "\n",
        "        mask_noise.normal_()\n",
        "        fake_mask = mask_gen(mask_noise)\n",
        "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
        "\n",
        "        impu_noise.uniform_()\n",
        "        imputed_data = imputer(real_data, real_mask, impu_noise)\n",
        "\n",
        "        update_data_critic(masked_real_data, masked_fake_data)\n",
        "        update_mask_critic(real_mask, fake_mask)\n",
        "        update_impu_critic(fake_data, imputed_data)\n",
        "\n",
        "        critic_updates += 1\n",
        "        if critic_updates == n_critic:\n",
        "            critic_updates = 0\n",
        "\n",
        "            # Update generators' parameters\n",
        "            for p in data_critic.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            for p in mask_critic.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            for p in impu_critic.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "            data_noise.normal_()\n",
        "            fake_data = data_gen(data_noise)\n",
        "\n",
        "            mask_noise.normal_()\n",
        "            fake_mask = mask_gen(mask_noise)\n",
        "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
        "\n",
        "            impu_noise.uniform_()\n",
        "            imputed_data = imputer(real_data, real_mask, impu_noise)\n",
        "\n",
        "            data_loss = -data_critic(masked_fake_data).mean()\n",
        "            mask_loss = -mask_critic(fake_mask).mean()\n",
        "            impu_loss = -impu_critic(imputed_data).mean()\n",
        "\n",
        "            mask_gen.zero_grad()\n",
        "            (mask_loss + data_loss * alpha).backward(retain_graph=True)\n",
        "            data_gen.zero_grad()\n",
        "            (data_loss + impu_loss * beta).backward(retain_graph=True)\n",
        "            mask_gen_optimizer.step()\n",
        "            data_gen_optimizer.step()\n",
        "\n",
        "            imputer.zero_grad()\n",
        "            impu_loss.backward()\n",
        "            imputer_optimizer.step()\n",
        "\n",
        "            for p in data_critic.parameters():\n",
        "                p.requires_grad_(True)\n",
        "            for p in mask_critic.parameters():\n",
        "                p.requires_grad_(True)\n",
        "            for p in impu_critic.parameters():\n",
        "                p.requires_grad_(True)\n",
        "\n",
        "    if plot_interval > 0 and (epoch ) % plot_interval == 0:\n",
        "        print(\"Iteration:\", epoch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            imputer.eval()\n",
        "\n",
        "            val_size = 3000\n",
        "            val_noise = torch.empty(val_size, num_features, device=device).uniform_()\n",
        "            val_data = torch.stack(masked_data.data)[:val_size].float()\n",
        "            val_mask = torch.stack(masked_data.mask)[:val_size].view(val_data.shape)\n",
        "            imputed_data = imputer(val_data.to(device),\n",
        "                                   val_mask.to(device).float(),\n",
        "                                   val_noise)\n",
        "            rmse = np.sqrt(np.mean((subset3[:val_size]-imputed_data.numpy())**2))\n",
        "            print(\"Validation RMSE:\", rmse)\n",
        "\n",
        "\n",
        "            test_size = 3000\n",
        "            # Show imputation results\n",
        "            # impu_noise.uniform_()\n",
        "            test_noise = torch.empty(test_size, num_features, device=device).uniform_()\n",
        "            # imputed_data = imputer(real_data, real_mask, impu_noise)\n",
        "            test_data = torch.stack(masked_data.data)[val_size:(val_size + test_size)].float()\n",
        "            test_mask = torch.stack(masked_data.mask)[val_size:(val_size + test_size)].view(test_data.shape)\n",
        "            imputed_data = imputer(test_data.to(device),\n",
        "                                   test_mask.to(device).float(),\n",
        "                                   test_noise)\n",
        "            #print(\"Real data:\")\n",
        "            #print(subset0[:test_size])\n",
        "            #print(\"Imputed data:\")\n",
        "            #print(imputed_data)\n",
        "            #print(\"Mask:\")\n",
        "            #print(torch.stack(masked_data.mask)[:test_size])\n",
        "            rmse = np.sqrt(np.mean((subset3[val_size:(val_size + test_size)]-imputed_data.numpy())**2))\n",
        "            print(\"Test RMSE:\", rmse)\n",
        "            # print('Epoch:', epoch)\n",
        "            # fig, ax = plt.subplots(figsize=(6, 3))\n",
        "            # plot_grid(ax, imputed_data, bbox, gap=2)\n",
        "            # plt.show()\n",
        "            # plt.close(fig)\n",
        "\n",
        "            imputer.train()\n",
        "t2=time.time()\n",
        "print(\"Run time: \",t2-t1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "Validation RMSE: 0.10677422\n",
            "Test RMSE: 0.1139572\n",
            "Iteration: 10\n",
            "Validation RMSE: 0.09948741\n",
            "Test RMSE: 0.10861789\n",
            "Iteration: 20\n",
            "Validation RMSE: 0.097926855\n",
            "Test RMSE: 0.109125584\n",
            "Iteration: 30\n",
            "Validation RMSE: 0.10959242\n",
            "Test RMSE: 0.12131825\n",
            "Iteration: 40\n",
            "Validation RMSE: 0.13550365\n",
            "Test RMSE: 0.14653997\n",
            "Iteration: 50\n",
            "Validation RMSE: 0.17662743\n",
            "Test RMSE: 0.18683586\n",
            "Iteration: 60\n",
            "Validation RMSE: 0.22640716\n",
            "Test RMSE: 0.23547938\n",
            "Iteration: 70\n",
            "Validation RMSE: 0.27672377\n",
            "Test RMSE: 0.28371757\n",
            "Iteration: 80\n",
            "Validation RMSE: 0.31930196\n",
            "Test RMSE: 0.32593176\n",
            "Iteration: 90\n",
            "Validation RMSE: 0.34835127\n",
            "Test RMSE: 0.3552013\n",
            "Iteration: 100\n",
            "Validation RMSE: 0.36006895\n",
            "Test RMSE: 0.36680388\n",
            "Iteration: 110\n",
            "Validation RMSE: 0.36232227\n",
            "Test RMSE: 0.36897057\n",
            "Iteration: 120\n",
            "Validation RMSE: 0.3563582\n",
            "Test RMSE: 0.3639042\n",
            "Iteration: 130\n",
            "Validation RMSE: 0.33881697\n",
            "Test RMSE: 0.34891152\n",
            "Iteration: 140\n",
            "Validation RMSE: 0.31841174\n",
            "Test RMSE: 0.3323738\n",
            "Run time:  111.46116042137146\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}